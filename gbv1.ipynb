{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf40941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: /home/kjnyua/miniconda3/envs/daystar\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this first if packages aren't installed\n",
    "!uv pip install transformers torch scikit-learn datasets pandas numpy emoji 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee30d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bec74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (39650, 3)\n",
      "Test data shape: (15581, 2)\n",
      "\n",
      "Training data columns: ['Tweet_ID', 'tweet', 'type']\n",
      "\n",
      "First few rows:\n",
      "      Tweet_ID                                              tweet  \\\n",
      "0  ID_0022DWKP  Had a dream i got raped last night. By a guy i...   \n",
      "1  ID_00395QYM  he thought the word raped means sex and told m...   \n",
      "2  ID_003EOSSF  She NOT TALKING TO ME I WAS RAPED BY 2 MEN 1 M...   \n",
      "3  ID_004BBHOD  I was sexually abused for 3 years at age 4 to ...   \n",
      "4  ID_004F7516  Chessy Prout can do better by telling the trut...   \n",
      "\n",
      "              type  \n",
      "0  sexual_violence  \n",
      "1  sexual_violence  \n",
      "2  sexual_violence  \n",
      "3  sexual_violence  \n",
      "4  sexual_violence  \n",
      "\n",
      "Label distribution:\n",
      "type\n",
      "sexual_violence                 32648\n",
      "Physical_violence                5946\n",
      "emotional_violence                651\n",
      "economic_violence                 217\n",
      "Harmful_Traditional_practice      188\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Train.csv\")\n",
    "test_df = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(train_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79212717-2367-47ac-b1d4-cfdc4cc0604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed!\n",
      "Sample processed text: Had a dream i got raped last night. By a guy i work with. Actually a guy i smoked with once at my house but he was doing too much tryna be sexual and it wasn’t even like that for me just wanted to smoke.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean tweet text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Normalize mentions and URLs (common in tweets)\n",
    "    text = text.replace(\"@\", \"@user\")\n",
    "    # Keep hashtags but normalize\n",
    "    text = text.replace(\"#\", \" #\")\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['tweeet'] = train_df['tweet'].apply(preprocess_text)\n",
    "test_df['tweet'] = test_df['tweet'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(\"Sample processed text:\", train_df['tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048085c2-3f22-4990-b58e-5d967f15b883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 5\n",
      "Labels: ['Harmful_Traditional_practice', 'Physical_violence', 'economic_violence', 'emotional_violence', 'sexual_violence']\n",
      "Label mapping: {'Harmful_Traditional_practice': 0, 'Physical_violence': 1, 'economic_violence': 2, 'emotional_violence': 3, 'sexual_violence': 4}\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "label_list = sorted(train_df['type'].unique().tolist())  # Sort for consistency\n",
    "n_labels = len(label_list)\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "train_df['label'] = train_df['type'].map(label2id)\n",
    "\n",
    "print(f\"Number of labels: {n_labels}\")\n",
    "print(f\"Labels: {label_list}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1475b2e3-be17-4874-bcdc-cf620faebd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (to handle imbalanced data):\n",
      "  Harmful_Traditional_practice: 42.181\n",
      "  Physical_violence: 1.334\n",
      "  economic_violence: 36.544\n",
      "  emotional_violence: 12.181\n",
      "  sexual_violence: 0.243\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Class weights (to handle imbalanced data):\")\n",
    "for label, weight in zip(label_list, class_weights):\n",
    "    print(f\"  {label}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c16e22-7b55-4e79-8e20-ec6b8177ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aed529e28854692a58150cbab82dbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90118ab6dcae472ab764f70216f3b0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text with proper padding\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['tweet'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=128,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing datasets...\")\n",
    "full_dataset = Dataset.from_pandas(train_df[['tweet', 'label']])\n",
    "full_dataset = full_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df[['tweet']])\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "print(\"Datasets prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1acdb-aeea-4cbf-8c6d-d1b6ee19cc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple trainer setup completed!\n",
      "Starting 5-fold cross-validation...\n",
      "This may take 15-30 minutes - grab a coffee! ☕\n",
      "Transformers version: 4.53.3\n",
      "\n",
      "### Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_36083/3033668514.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/kjnyua/miniconda3/envs/daystar/lib/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='5949' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/5949 02:10 < 35:45:43, 0.05 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alternative Cell 8: No Custom Trainer (Simpler)\n",
    "# Just use the standard Trainer - still very effective!\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "print(\"Simple trainer setup completed!\")\n",
    "\n",
    "# Alternative Cell 9: Cross-validation without custom trainer\n",
    "print(\"Starting 5-fold cross-validation...\")\n",
    "print(\"This may take 15-30 minutes - grab a coffee! ☕\")\n",
    "\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "oof_preds = np.zeros((len(train_df), n_labels))\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "    print(f\"\\n### Fold {fold+1}/5\")\n",
    "    \n",
    "    train_ds = full_dataset.select(train_idx)\n",
    "    val_ds = full_dataset.select(val_idx)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"vinai/bertweet-base\",\n",
    "        num_labels=n_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Simple TrainingArguments - no custom trainer needed\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"out_fold{fold}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        seed=SEED,\n",
    "        logging_steps=100,\n",
    "        report_to=None,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "    \n",
    "    # Use standard Trainer (no class weights, but still very effective)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        eval_result = trainer.evaluate()\n",
    "        fold_accuracy = eval_result['eval_accuracy']\n",
    "        \n",
    "        # Get predictions for out-of-fold validation\n",
    "        preds = trainer.predict(val_ds).predictions\n",
    "        oof_preds[val_idx] = preds\n",
    "        \n",
    "        fold_scores.append(fold_accuracy)\n",
    "        print(f\"Fold {fold+1} accuracy: {fold_accuracy:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"Out of memory error in fold {fold+1}. Trying with smaller batch size...\")\n",
    "            \n",
    "            # Clear memory first\n",
    "            del model, trainer\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # Recreate model with smaller batch size\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"vinai/bertweet-base\",\n",
    "                num_labels=n_labels,\n",
    "                id2label=id2label,\n",
    "                label2id=label2id,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                output_dir=f\"out_fold{fold}\",\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=8,  # Reduced batch size\n",
    "                per_device_eval_batch_size=8,\n",
    "                num_train_epochs=3,\n",
    "                weight_decay=0.01,\n",
    "                seed=SEED,\n",
    "                logging_steps=100,\n",
    "                report_to=None,\n",
    "                save_strategy=\"no\",\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            eval_result = trainer.evaluate()\n",
    "            fold_accuracy = eval_result['eval_accuracy']\n",
    "            preds = trainer.predict(val_ds).predictions\n",
    "            oof_preds[val_idx] = preds\n",
    "            fold_scores.append(fold_accuracy)\n",
    "            print(f\"Fold {fold+1} accuracy (reduced batch): {fold_accuracy:.4f}\")\n",
    "            \n",
    "            # Clear memory again\n",
    "            del model, trainer\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        else:\n",
    "            print(f\"Error in fold {fold+1}: {e}\")\n",
    "            # Add a dummy score to continue\n",
    "            fold_scores.append(0.0)\n",
    "\n",
    "print(f\"\\n🎉 Cross-validation completed!\")\n",
    "print(f\"Fold scores: {[f'{score:.4f}' for score in fold_scores if score > 0]}\")\n",
    "\n",
    "if len([s for s in fold_scores if s > 0]) > 0:\n",
    "    valid_scores = [s for s in fold_scores if s > 0]\n",
    "    print(f\"Average CV accuracy: {np.mean(valid_scores):.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ No successful folds completed. Check your setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26478dd5-010a-4366-b762-558958726554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daystar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
